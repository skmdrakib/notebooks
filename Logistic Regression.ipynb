{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification is a very common and important variant among Machine Learning Problems. Many Machine Algorithms have been framed to tackle classification (discrete not continuous) problems. Examples of classification based predictive analytics problems are:\n",
    "\n",
    "* Diabetic Retinopathy: Given a retinal image, classify the image (eye) as Diabetic or Non-Diabetic.\n",
    "* Sentiment Analysis: Given a sentence, analyze the sense of the sentence (for ex. happiness/sadness, praise/insult, etc.)\n",
    "* Digit Recognition: Given an image of a digit, recognize the digit (0–9). This is an example of Multi-Class Classification.\n",
    "\n",
    "and many more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology related to classification\n",
    "\n",
    "- Classifier: An algorithm that maps the input data to a specific category.\n",
    "- Classification model: A classification model tries to draw some conclusion from the input values given for training. It will predict the class labels/categories for the new data.\n",
    "- Feature: A feature is an individual measurable property of a phenomenon being observed.\n",
    "- Binary Classification: Classification task with two possible outcomes. Eg: Gender classification (Male / Female)\n",
    "- Multi class classification: Classification with more than two classes. In multi class classification each sample is assigned to one and only one target label. Eg: An animal can be cat or dog but not both at the same time\n",
    "- Multi label classification: Classification task where each sample is mapped to a set of target labels (more than one class). Eg: A news article can be about sports, a person, and location at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Despire being called a regression, logistic regression is actually a widely used supervised classification technique. \n",
    "Allows us to predict the probability that an observation is of a certain class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Some Python Libraries</h2>\n",
    "\n",
    "<p style=\"text-align: justify;\">In the first place, Let's define some libraries to help us in the manipulation the data set, such as `pandas`, `numpy`, `matplotlib`, `seaborn`. In this tutorial, we are implementing a Logistic Regression with `sikit-learn`. The goal here is to be as simple as possible! So to help you with this task, we implementing the Logistic regression using ready-made libraries and their functinality.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Ad click Project </h4>\n",
    "\n",
    "Let us now start implementing what we learned from the previous section into python codes. We will use a website data of Customers to understand which customer will be click the AD, by the end of this section we will be able to make predictions using our \"home-made\" Logistic Regression.\n",
    "\n",
    "This data set contains the following features:\n",
    "\n",
    "* '`Daily Time Spent on Site`': consumer time on site in minutes\n",
    "* '`Age`': cutomer age in years\n",
    "* '`Area Income`': Avg. Income of geographical area of consumer\n",
    "* '`Daily Internet Usage`': Avg. minutes a day consumer is on the internet\n",
    "* '`Ad Topic Line`': Headline of the advertisement\n",
    "* '`City`': City of consumer\n",
    "* '`Male`': Whether or not consumer was male\n",
    "* '`Country`': Country of consumer\n",
    "* '`Timestamp`': Time at which consumer clicked on Ad or closed window\n",
    "* '`Clicked on Ad`': 0 or 1 indicated clicking on Ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the dataset\n",
    "import os\n",
    "df=pd.read_csv(os.getcwd()+\"\\\\Datasets\\\\Web_data_v3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.duplicated().value_counts())\n",
    "df.drop_duplicates(inplace = True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Data Exploration Results\n",
    "Based on the basic exploration, this dataset have 6657 rows and 14 columns also we see there are no missing values in this dataset and no duplicate rows.\n",
    "\n",
    "#### The selected columns in this step are not final, further study will be done and then a final list will be created\n",
    "- VistID: Qualitative\n",
    "- Time_Spent: Continuous\n",
    "- Age: Continuous\n",
    "- Avg_Income: Continuous\n",
    "- Internet_Usage: Continuous\n",
    "- Ad_Topic: Categorical\n",
    "- Country_Name: Categorical\n",
    "- City_code: Categorical\n",
    "- Male: Categorical\n",
    "- Time_Period: Categorical\n",
    "- Weekday: Categorical\n",
    "- Month: Categorical\n",
    "- Year: Categorical\n",
    "- Clicked: Categorical. This is the Target Variable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.rc(\"font\", size=14)\n",
    "ax = sns.countplot(y ='Clicked',data=df)\n",
    "total = len(df['Clicked'])\n",
    "for p in ax.patches:\n",
    "    percentage = '{:.2f}%'.format(100 * p.get_width()/total)\n",
    "    x = p.get_x() + p.get_width() + 0.02\n",
    "    y = p.get_y() + p.get_height()/2\n",
    "    ax.annotate(percentage, (x, y))\n",
    "plt.rc(\"font\", size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Over here we see that class 0 have 3619 rows and class 1 have 3038 rows.\n",
    "- After checking the percentage of those class it didn't imply that this data is imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variables list based on their types.\n",
    "categorical_col=[]\n",
    "numerical_col=[]\n",
    "\n",
    "for col in df.columns[2:-1]:\n",
    "    if df[col].dtype ==\"object\":\n",
    "        categorical_col.append(col)\n",
    "    elif df[col].dtype ==\"int64\" or df[col].dtype ==\"float64\":\n",
    "        numerical_col.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual exploration (Categorical Vs Categorical) -- Bar Charts or Grouped Bar Charts\n",
    "When the target variable is Categorical and the predictor is also Categorical then we explore the correlation between them visually using barplots and Grouped Bar Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_col:\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    sns.barplot(data=df,x=col,y=\"Clicked\",palette='hot')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(45, 6))\n",
    "sns.barplot(data=df,x=\"Ad_Topic\",y=\"Clicked\",palette='hot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Grouped bar plots for each categorical predictor against the Target Variable \"Clicked\"\n",
    "for col in categorical_col:\n",
    "    CrossTabResult=pd.crosstab(index=df[col], columns=df[\"Clicked\"])\n",
    "    CrossTabResult.plot.bar(color=['lightblue','blue'], figsize=(15,6))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From above plots we learn that\n",
    "- In `Ad_Topic` all the others ads distrivution is same except 'product_2' ad is lowest and 'product_3' ad is highest but in case of 'product_3' the click rate is highest than any other product\n",
    "- In `City_Code` most of the country have only one cities data that's why the distribution is high of 'city_1' but we also see that In case any countries have more than one city the 'city_1' people have more clicked the ad other than other cities.\n",
    "- In `Male` columns we see the similar distribution and clicked rate only count of male visitor are more than female that's why click rate also higher than female.\n",
    "- In `Time_Prediod` we see most people visit the website in 'early_morning' similary the click rate of ad is higher in that time.\n",
    "- In `week_day` the distibution and click rate is same.\n",
    "- In `Month` column we see more visitor in february and may month and similarly the ad click rate also high in those months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why do we need Feature Selection ? \n",
    "\n",
    "# 100, 200, 400 - imagination - not all of these features are significant to make the prediction\n",
    "# Larger Dimension - rows, columns -> more complex models are needed\n",
    "# Overfitting - model will start using insignificant features to explain randomness of the error\n",
    "\n",
    "\n",
    "# Feature selection -> relevancy of feature w.r.t the output\n",
    "# Redundant with other input data\n",
    "\n",
    "# features - numerical, categorical\n",
    "# output - numerical and categorical\n",
    "#  I/P            O/P\n",
    "# numerical and numerical\n",
    "# numerical and categorical\n",
    "# categorical and numerical\n",
    "# categorical and categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Feature Selection (Categorical Vs Categorical) using Chi-Square Test\n",
    "Chi-Square test is conducted to check the correlation between two categorical variables\n",
    "\n",
    "* Assumption(H0): The two columns are NOT related to each other\n",
    "* Result of Chi-Sq Test: The Probability of H0 being True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis Testing\n",
    "\n",
    "# Null hypothesis  - Opposite the idea ( There is no relationship )\n",
    "# Alternative Hypothesis - Acceptance of the idea \n",
    "\n",
    "\n",
    "# p-value < 0.05 - Reject the null hypothesis - accepting the idea / accepting the alternative hypothesis\n",
    "# p-value >=0.05 - Cannot reject the NH - accept the opposition f the idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing a function to find the correlation of all categorical variables with the Target variable\n",
    "def FunctionChisq(inpData, TargetVariable, CategoricalVariablesList):\n",
    "    from scipy.stats import chi2_contingency\n",
    "    \n",
    "    # Creating an empty list of final selected predictors\n",
    "    SelectedPredictors=[]\n",
    "    \n",
    "    print('##### chi-square Results ##### \\n')\n",
    "    for predictor in CategoricalVariablesList:\n",
    "        CrossTabResult=pd.crosstab(index=inpData[TargetVariable], columns=inpData[predictor])\n",
    "        ChiSqResult = chi2_contingency(CrossTabResult)\n",
    "        \n",
    "        # If the ChiSq P-Value is <0.05, that means we reject H0, -> p low, null go\n",
    "        if ( ChiSqResult[1] < 0.05 ):\n",
    "            SelectedPredictors.append(predictor)\n",
    "            print(predictor, 'is correlated with', TargetVariable, '| P-Value:', ChiSqResult[1])\n",
    "        else:\n",
    "            print(predictor, 'is NOT correlated with', TargetVariable, '| P-Value:', ChiSqResult[1])\n",
    "            \n",
    "    #return(SelectedPredictors)\n",
    "    return CrossTabResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = FunctionChisq(inpData=df, \n",
    "              TargetVariable=\"Clicked\",\n",
    "              CategoricalVariablesList= categorical_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We see 'Ad_Topic', 'Country_Name', 'City_code', 'Male', 'Time_Period' are imporatant varibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual exploration (Continuous Vs Categorical) -- Histogram and Box/Violin Plots\n",
    "When the target variable is Categorical and the predictor is also Continuous then we explore the correlation between them visually using Histogram and Box Plots or Violin Plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numerical_col:\n",
    "    df.hist(col, figsize=(12,6))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numerical_col:\n",
    "    plt.figure(figsize=(14,6))\n",
    "    sns.boxplot(x=\"Clicked\", y=col, data=df)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numerical_col:\n",
    "    plt.figure(figsize=(14,6))\n",
    "    sns.violinplot(x=\"Clicked\", y=col, data=df)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From above plots we learn that\n",
    "- In `Age` higher the age less frequently they visited the website and mid-aged people are more frequent clicked the ad.\n",
    "- In `Avg_Income` the distribution id left skewed and higher the income lesser the frequent they clicked the ad.\n",
    "- In `Internet_Usage` people who spent maximum time in the website are less frequent to clicked the ad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Feature Selection (Categorical Vs Continuous) using ANOVA test\n",
    "Analysis of variance(ANOVA) is performed to check if there is any relationship between the given continuous and categorical variable\n",
    "\n",
    "- Assumption(H0): There is NO relation between the given variables (i.e. The average(mean) values of the numeric Predictor variable is same for all the groups in the categorical Target variable)\n",
    "- ANOVA Test result: Probability of H0 being true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to find the statistical relationship with all the categorical variables\n",
    "def FunctionAnova(inpData, TargetVariable, ContinuousPredictorList):\n",
    "    from scipy.stats import f_oneway\n",
    "\n",
    "    # Creating an empty list of final selected predictors\n",
    "    SelectedPredictors=[]\n",
    "    \n",
    "    print('##### ANOVA Results ##### \\n')\n",
    "    for predictor in ContinuousPredictorList:\n",
    "        CategoryGroupLists=inpData.groupby(TargetVariable)[predictor].apply(list)\n",
    "        AnovaResults = f_oneway(*CategoryGroupLists)\n",
    "        \n",
    "        # If the ANOVA P-Value is <0.05, that means we reject H0\n",
    "        if (AnovaResults[1] < 0.05):\n",
    "            print(predictor, 'is correlated with', TargetVariable, '| P-Value:', AnovaResults[1])\n",
    "            SelectedPredictors.append(predictor)\n",
    "        else:\n",
    "            print(predictor, 'is NOT correlated with', TargetVariable, '| P-Value:', AnovaResults[1])\n",
    "    \n",
    "    return (SelectedPredictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the function to check which categorical variables are correlated with target\n",
    "FunctionAnova(inpData=df, TargetVariable=\"Clicked\", ContinuousPredictorList=numerical_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From ANOVA test we saw 'Age', 'Avg_Income', 'Internet_Usage' are the variables have some impact on target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "sns.pairplot(df, hue='Clicked')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Correlation between two variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data=df[['Age', 'Time_Spent', 'Avg_Income', 'Internet_Usage', 'Clicked']]\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_data.corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr=corr_data.corr()\n",
    "corr['Clicked'][abs(corr['Clicked']) > 0.5 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Logistic Regression Model\n",
    "The assumptions made by logistic regression about the distribution and relationships in your data are much the same as the assumptions made in linear regression.\n",
    "\n",
    "Ultimately in predictive modeling machine learning projects you are more focused on making accurate predictions rather than interpreting the results. As such, you can break some assumptions as long as the model is robust and performs well.\n",
    "\n",
    "- **Binary Output Variable:** This might be obvious as we have already mentioned it, but logistic regression is intended for binary (two-class) classification problems. It will predict the probability of an instance belonging to the default class, which can be snapped into a 0 or 1 classification.\n",
    "- **Remove Noise:** Logistic regression assumes no error in the output variable (y), consider removing outliers and possibly misclassified instances from your training data.\n",
    "- **Gaussian Distribution:** Logistic regression is a linear algorithm (with a non-linear transform on output). It does assume a linear relationship between the input variables with the output. Data transforms of your input variables that better expose this linear relationship can result in a more accurate model. For example, you can use log, root, Box-Cox and other univariate transforms to better expose this relationship.\n",
    "- **Remove Correlated Inputs:** Like linear regression, the model can overfit if you have multiple highly-correlated inputs. Consider calculating the pairwise correlations between all inputs and removing highly correlated inputs.\n",
    "- **Fail to Converge:** It is possible for the expected likelihood estimation process that learns the coefficients to fail to converge. This can happen if there are many highly correlated inputs in your data or the data is very sparse (e.g. lots of zeros in your input data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Requirement - Feature Selection has happened - Feature Processing\n",
    "# Categorical Data -> Categorical Transformation\n",
    "# 2 - type of categorical data ->  Ordinal, Nominal \n",
    "\n",
    "# Ordinal categorical data has an order ( Negative, Neutral, Positive ) - numerical conversion\n",
    "# Nominal Categorical data - has no order ( Red, Green Blue )  - Encoding of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ( Negative, Neutral, Positive ) -> ( -1 , 0 , +1 )\n",
    "# Color\n",
    "\n",
    "# Red\n",
    "# Green\n",
    "# Blue\n",
    "# Green\n",
    "# Green\n",
    "# Blue\n",
    "# Blue\n",
    "# Red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add n number of columns , where n  = number of unique categories in the column\n",
    "# for each existing value in the original categorical column - replace the new columns created with 1, rest all new columns will be 0\n",
    "\n",
    "# Red , Green , Blue \n",
    "# 1 ,   0 ,     0\n",
    "# 0 ,   1 ,     0\n",
    "# 0 ,   0  ,    1\n",
    "# 0 ,   1 ,     0\n",
    "# 0 ,   1 ,     0\n",
    "# 0 ,   0  ,    1\n",
    "# 0 ,   0  ,    1\n",
    "# 1 ,   0 ,     0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encodeing \n",
    "\n",
    "In machine learning, we usually deal with datasets that contain multiple labels in one or more than one columns. These labels can be in the form of words or numbers. To make the data understandable or in human-readable form, the training data is often labelled in words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply Label encoder to df_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoders = {}\n",
    "for column in categorical_col:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    df[column] = label_encoders[column].fit_transform(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the final data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning: Splitting the data into Training and Testing sample\n",
    "We dont use the full data for creating the model. Some data is randomly selected and kept aside for checking how good the model is. This is known as Testing Data and the remaining data is called Training data on which the model is built. Typically 70% of data is used as Training data and the rest 30% is used as Tesing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(['VistID', 'Year', 'Clicked'], axis=1)\n",
    "y = df['Clicked']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization/Normalization of data\n",
    "You can choose not to run this step if you want to compare the resultant accuracy of this transformation with the accuracy of raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ss = StandardScaler()\n",
    "\n",
    "# df[col] = ss.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "ct = make_column_transformer(\n",
    "    (MinMaxScaler(), categorical_col),\n",
    "    (StandardScaler(), numerical_col[:-1]),\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "X_train = ct.fit_transform(X_train)\n",
    "X_test = ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing for ML - \n",
    "\n",
    "# 1. Scaling / Standardisation\n",
    "# 2. Train and Testing Split\n",
    "# 3. Feature Selection - Correlation of num to num, num to cat, cat to cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination\n",
    "\n",
    "The idea behind recursive feature elimination (RFE) is to train a model that contains some parameters (also called weights or coefficients) like linear regression or support vector machines repeatedly. The first time we train the model, we include all the features. Then, we find the\n",
    "feature with the smallest parameter (notice that this assumes the features are either rescaled or standardized), meaning it is less important, and remove the feature from the feature set.\n",
    "\n",
    "The obvious question then is: how many features should we keep? We can (hypothetically) repeat this loop until we only have one feature left. A better approach requires that we include a new concept called cross-validation (CV). but here is the general idea.\n",
    "\n",
    "Given data containing 1) a target we want to predict and 2) a feature matrix, first we split the data into two groups: a training set and a test set. Second, we train our model using the training set. Third, we pretend that we do not know the target of the test set, and apply our model to the test set’s features in order to predict the values of the test set. Finally, we compare our predicted target values with the true target values to evaluate our model.\n",
    "\n",
    "We can use CV to find the optimum number of features to keep during RFE. Specifically, in RFE with CV after every iteration, we use cross-validation to evaluate our model. If CV shows that our model improved after we eliminated a feature, then we continue on to the next loop. However, if CV shows that our model got worse after we eliminated a feature, we put that feature back into the feature set and select those features as the best.\n",
    "\n",
    "In scikit-learn, RFE with CV is implemented using RFECV and contains a number of important parameters. The estimator parameter determines the type of model we want to train (e.g., linear regression). The step regression sets the number or proportion of features to drop during each loop. The scoring parameter sets the metric of quality we use to evaluate our model during cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV # Recursive feature elimination with cross validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "rfe = RFECV(logreg, step=1, scoring=\"neg_mean_squared_error\")\n",
    "rfe = rfe.fit(X_train, y_train.values.ravel())\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have conducted RFE, we can see the number of features we should keep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of best features\n",
    "rfe.n_features_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see which of those features we should keep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which categories are best\n",
    "rfe.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns[rfe.support_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even view the rankings of the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank features best (1) to worst\n",
    "rfe.ranking_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Binary Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dispire having \"regression\" in its name, a logistic regression is actually a widely used binary lassifier (i.e. the target vector can only take two values). In a logistic regression, a linear model (e.g. $\\beta_0 + \\beta_i x$) is included in a logistic (also called sigmoid) function, $\\frac{1}{1+e^{-z }}$, such that:\n",
    "$$\n",
    "P(y_i = 1 | X) = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1x)}}\n",
    "$$\n",
    "where $P(y_i = 1 | X)$ is the probability of the ith obsevation's target, $y_i$ being class 1, X is the training data, $\\beta_0$ and $\\beta_1$ are the parameters to be learned, and e is Euler's number. The effect of the logistic function is to constrain the value of the function's output to between 0 and 1 so that i can be interpreted as a probability. If $P(y_i = 1 | X)$ is greater than 0.5, class 1 is predicted; otherwise class 0 is predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the logistic regression algorithm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(random_state=0)\n",
    "\n",
    "# Customisation of your logistic regression model\n",
    "print(logreg)\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)\n",
    "f3 = pd.DataFrame(y_pred)\n",
    "f3['Clicked'] = y_test.values\n",
    "print(\"Accuracy of Logistic Regression is : {}%\".format(round(f3.loc[f3[0]==f3['Clicked']].shape[0] / f3.shape[0] * 100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages to measure model performace\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, accuracy_score, f1_score, precision_score, recall_score,auc\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Performance Measurement<h2>\n",
    "\n",
    "#### 1. Confusion Matrix\n",
    "- Each row: actual class\n",
    "- Each column: predicted class\n",
    "\n",
    "#### 2. Precision\n",
    "\n",
    "**Precision** measures the accuracy of positive predictions. Also called the `precision` of the classifier\n",
    "\n",
    "$$\\textrm{precision} = \\frac{\\textrm{True Positives}}{\\textrm{True Positives} + \\textrm{False Positives}}$$\n",
    "\n",
    "#### 3. Recall\n",
    "\n",
    "`Precision` is typically used with `recall` (`Sensitivity` or `True Positive Rate`). The ratio of positive instances that are correctly detected by the classifier.\n",
    "\n",
    "$$\\textrm{recall} = \\frac{\\textrm{True Positives}}{\\textrm{True Positives} + \\textrm{False Negatives}}$$ \n",
    "\n",
    "#### 4. F1 Score\n",
    "\n",
    "$F_1$ score is the harmonic mean of precision and recall. Regular mean gives equal weight to all values. Harmonic mean gives more weight to low values.\n",
    "\n",
    "\n",
    "$$F_1=\\frac{2}{\\frac{1}{\\textrm{precision}}+\\frac{1}{\\textrm{recall}}}=2\\times \\frac{\\textrm{precision}\\times \\textrm{recall}}{\\textrm{precision}+ \\textrm{recall}}=\\frac{TP}{TP+\\frac{FN+FP}{2}}$$\n",
    "\n",
    "The $F_1$ score favours classifiers that have similar precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifaction Model - My model's accuracy is 93% what does it mean in classification and why it is wrong?\n",
    "\n",
    "# Actual - True , False ( 2 ) \n",
    "# Predicted - True, False  ( 2 ) \n",
    "# Pred  Actual\n",
    "# True  True -> True Positive Prediction\n",
    "# True  False -> False Positive Prediction\n",
    "# False True -> False negative\n",
    "# False False -> True negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COVID Question ? \n",
    "\n",
    "# initial phase of the COVID detection\n",
    "# Focused on -> True positive + False Negative => Actual positive\n",
    "# Recall measure\n",
    "\n",
    "# Later stage of COVID detection\n",
    "# focused on -> True Positive + False Positive => Positive predictions made by me\n",
    "# Precision Measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Checking Confusion Metrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# confusion metrics\n",
    "plt.figure(figsize=(10,8))\n",
    "ax = sns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'viridis_r', fmt = 'd')\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision / Recall Tradeoff\n",
    "\n",
    "Increasing precision reduced recall and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(thresholds, recalls[:-1], \"g--\", label=\"Recall\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.title(\"Precisions/recalls tradeoff\")\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, logreg.predict(X_test))\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(precisions, recalls)\n",
    "plt.xlabel(\"Precision\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.title(\"PR Curve: precisions/recalls tradeoff\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this chart, you can select the threshold value that gives you the best precision/recall tradeoff for your task.\n",
    "\n",
    "Some tasks may call for higher precision (accuracy of positive predictions). Like designing a classifier that picks up adult contents to protect kids. This will require the classifier to set a high bar to allow any contents to be consumed by children.\n",
    "\n",
    "Some tasks may call for higher recall (ratio of positive instances that are correctly detected by the classifier). Such as detecting shoplifters/intruders on surveillance images - Anything that remotely resemble \"positive\" instances to be picked up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The Receiver Operating Characteristics (ROC) Curve</h2>\n",
    "\n",
    "Instead of plotting precision versus recall, the ROC curve plots the `true positive rate` (another name for recall) against the `false positive rate`. The `false positive rate` (FPR) is the ratio of negative instances that are incorrectly classified as positive. It is equal to one minus the `true negative rate`, which is the ratio of negative instances that are correctly classified as negative.\n",
    "\n",
    "The TNR is also called `specificity`. Hence the ROC curve plots `sensitivity` (recall) versus `1 - specificity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logreg.predict(X_test))\n",
    "plt.figure(figsize=(12,8)); \n",
    "plot_roc_curve(fpr, tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use PR curve whenever the **positive class is rare** or when you care more about the false positives than the false negatives\n",
    "\n",
    "Use ROC curve whenever the **negative class is rare** or when you care more about the false negatives than the false positives\n",
    "\n",
    "\n",
    "In the example above, the ROC curve seemed to suggest that the classifier is good. However, when you look at the PR curve, you can see that there are room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Multiclass Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "logistic_regression = LogisticRegression(random_state=0, multi_class=\"ovr\")\n",
    "#logistic_regression_MNL = LogisticRegression(random_state=0, multi_class=\"multinomial\")\n",
    "\n",
    "model = logistic_regression.fit(features_standardized, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On their own, logistic regressions are only binary classifiers, meaning they cannot handle target vectors with more than two classes. However, two clever extensions to logistic regression do just that. First, in one-vs-rest logistic regression (OVR) a separate model is trained for each class predicted whether an observation is that class or not (thus making it a binary classification problem). It assumes that each observation problem (e.g. class 0 or not) is independent\n",
    "\n",
    "Alternatively in multinomial logistic regression (MLR) the logistic function we saw in Recipe 15.1 is replaced with a softmax function:\n",
    "$$\n",
    "P(y_I = k | X) = \\frac{e^{\\beta_k x_i}}{\\sum_{j=1}^{K}{e^{\\beta_j x_i}}}\n",
    "$$\n",
    "where $P(y_i = k | X)$ is the probability of the ith observation's target value, $y_i$, is class k, and K is the total number of classes. One practical advantage of the MLR is that its predicted probabilities using `predict_proba` method are more reliable\n",
    "\n",
    "We can switch to an MNL by setting `multi_class='multinomial'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Variance Through Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "logistic_regression = LogisticRegressionCV(\n",
    "    penalty='l2', Cs=10, random_state=0, n_jobs=-1)\n",
    "\n",
    "model = logistic_regression.fit(features_standardized, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is a method of penalizing complex models to reduce their variance. Specifically, a penalty term is added to the loss function we are trying to minimize typically the L1 and L2 penalties\n",
    "\n",
    "In the L1 penalty:\n",
    "$$\n",
    "\\alpha \\sum_{j=1}^{p}{|\\hat\\beta_j|}\n",
    "$$\n",
    "where $\\hat\\beta_j$ is the parameters of the jth of p features being learned and $\\alpha$ is a hyperparameter denoting the regularization strength.\n",
    "\n",
    "With the L2 penalty:\n",
    "$$\n",
    "\\alpha \\sum_{j=1}^{p}{\\hat\\beta_j^2}\n",
    "$$\n",
    "higher values of $\\alpha$ increase the penalty for larger parameter values(i.e. more complex models). scikit-learn follows the common method of using C instead of $\\alpha$ where C is the inverse of the regularization strength: $C = \\frac{1}{\\alpha}$. To reduce variance while using logistic regression, we can treat C as a hyperparameter to be tuned to find thevalue of C that creates the best model. In scikit-learn we can use the `LogisticRegressionCV` class to efficiently tune C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Classifier on Very Large Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "logistic_regression = LogisticRegression(random_state=0, solver=\"sag\") # stochastic average gradient (SAG) solver\n",
    "model = logistic_regression.fit(features_standardized, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn's `LogisticRegression` offers a number of techniques for training a logistic regression, called solvers. Most of the time scikit-learn will select the best solver automatically for us or warn us we cannot do something with that solver.\n",
    "\n",
    "Stochastic averge gradient descent allows us to train a model much faster than other solvers when our data is very large. However, it is also very sensitive to feature scaling, so standardizing our features is particularly important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Imbalanced Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data[40:, :]\n",
    "target = iris.target[40:]\n",
    "\n",
    "target = np.where((target == 0), 0, 1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "logistic_regression = LogisticRegression(random_state=0, class_weight=\"balanced\")\n",
    "model = logistic_regression.fit(features_standardized, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LogisticRegression` comes with a built in method of handling imbalanced classes.\n",
    "`class_weight=\"balanced\"` will automatically weigh classes inversely proportional to their frequency:\n",
    "$$\n",
    "w_j = \\frac{n}{kn_j}\n",
    "$$\n",
    "where $w_j$ is the weight to class j, n is the number of observations, $n_j$ is the number of observations in class j, and k is the total number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
